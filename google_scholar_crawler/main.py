#!/usr/bin/env python3
"""
æŠ“å– Google Scholar æ•°æ®å¹¶ä¿å­˜åˆ° results/ ç›®å½•
"""

import os
import sys
import time
import random
import json
from datetime import datetime

from scholarly import scholarly, ProxyGenerator, MaxTriesExceededException

# --------------------------- ä»£ç†åˆå§‹åŒ– ---------------------------------
def init_scholar_session() -> None:
    """åˆå§‹åŒ– ProxyGeneratorï¼›è‹¥ä¸€ç§æ–¹å¼å¤±è´¥åˆ™è‡ªåŠ¨å°è¯•ä¸‹ä¸€ç§ã€‚"""
    pg = ProxyGenerator()

    # 1) å…è´¹è½®æ¢ä»£ç†
    if pg.FreeProxies():
        scholarly.use_proxy(pg)
        print("âœ… ä½¿ç”¨ FreeProxies() è½®æ¢ä»£ç†")
        return

    # 2) æœ¬åœ° Tor
    if pg.Tor_Internal(tor_cmd="tor"):        # éœ€æœ¬æœºå·²å®‰è£… & è¿è¡Œ tor
        scholarly.use_proxy(pg)
        print("âœ… ä½¿ç”¨æœ¬åœ° Tor ä»£ç†")
        return

    # 3) ScraperAPIï¼ˆæˆ–ä½ è‡ªå·±çš„ä»˜è´¹ä»£ç†ï¼‰
    key = os.getenv("SCRAPERAPI_KEY")
    if key and pg.ScraperAPI(key):
        scholarly.use_proxy(pg)
        print("âœ… ä½¿ç”¨ ScraperAPI ä»£ç†")
        return

    raise RuntimeError("âŒ æ‰€æœ‰ä»£ç†æ–¹å¼å‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†é…ç½®")

# --------------------------- æŠ“å–å‡½æ•° -----------------------------------
def fetch_author(author_id: str, max_retries: int = 4) -> dict:
    """æŠ“å–æŒ‡å®šä½œè€…ä¿¡æ¯ï¼›å¤±è´¥æ—¶è‡ªåŠ¨æ¢ä»£ç†å¹¶é‡è¯•ã€‚"""
    for attempt in range(1, max_retries + 1):
        try:
            author = scholarly.search_author_id(author_id)
            scholarly.fill(
                author,
                sections=["basics", "indices", "counts", "publications"],
            )
            return author

        except MaxTriesExceededException as e:
            print(f"[{attempt}/{max_retries}] è¯·æ±‚è¢«å°ï¼š{e}")
        except Exception as e:
            print(f"[{attempt}/{max_retries}] å…¶ä»–å¼‚å¸¸ï¼š{e}")

        # è‹¥æœªè¿”å›ï¼Œè¯´æ˜æŠ“å–å¤±è´¥ï¼Œè¿›è¡Œé€€é¿å¹¶åˆ·æ–°ä»£ç†
        wait = 10 * attempt + random.uniform(0, 5)
        print(f"â³ {wait:.1f}s åé‡è¯•â€¦")
        time.sleep(wait)        # æŒ‡æ•°é€€é¿
        init_scholar_session()  # é‡æ–°æŒ‚ä»£ç†

    raise RuntimeError(f"âŒ é‡è¯• {max_retries} æ¬¡åä»æ— æ³•æŠ“å– {author_id}")

# --------------------------- ä¸»æµç¨‹ -------------------------------------
def main() -> None:
    author_id = os.getenv("GOOGLE_SCHOLAR_ID")
    if not author_id:
        sys.exit("ğŸŒŸ è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ GOOGLE_SCHOLAR_ID")

    # ç¬¬ä¸€æ¬¡åˆå§‹åŒ–ä»£ç†
    init_scholar_session()

    # æŠ“æ•°æ®
    author = fetch_author(author_id)
    author["updated"] = datetime.now().isoformat(timespec="seconds")
    # æŠŠ publications ä» list è½¬æˆ dictï¼Œkey=author_pub_id
    author["publications"] = {p["author_pub_id"]: p for p in author["publications"]}

    # è¾“å‡ºåˆ°ç»ˆç«¯
    print(json.dumps(author, indent=2, ensure_ascii=False))

    # ä¿å­˜æ–‡ä»¶
    os.makedirs("results", exist_ok=True)
    with open("results/gs_data.json", "w", encoding="utf-8") as f:
        json.dump(author, f, ensure_ascii=False)

    # shields.io å°å¾½ç« æ•°æ®
    shieldio_data = {
        "schemaVersion": 1,
        "label": "citations",
        "message": f"{author['citedby']}",
    }
    with open("results/gs_data_shieldsio.json", "w", encoding="utf-8") as f:
        json.dump(shieldio_data, f, ensure_ascii=False)

    print("âœ… æŠ“å–å®Œæˆï¼Œç»“æœå·²å†™å…¥ results/")

# --------------------------- å…¥å£ ---------------------------------------
if __name__ == "__main__":
    main()
